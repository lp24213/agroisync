name: Backup PostgreSQL

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to backup'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging
          - development

jobs:
  backup:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18.17.0'
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}
        
    - name: Create backup directory
      run: mkdir -p backups
      
    - name: Create PostgreSQL backup
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="agrotm_db_${TIMESTAMP}.sql"
        
        # Create backup using pg_dump
        pg_dump \
          --host=${{ secrets.DB_HOST }} \
          --port=${{ secrets.DB_PORT }} \
          --username=${{ secrets.DB_USERNAME }} \
          --dbname=${{ secrets.DB_NAME }} \
          --verbose \
          --clean \
          --no-owner \
          --no-privileges \
          --file="backups/${BACKUP_FILE}"
      env:
        PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        
    - name: Compress backup
      run: |
        cd backups
        gzip *.sql
        
    - name: Upload backup to S3
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="agrotm_db_${TIMESTAMP}.sql.gz"
        
        aws s3 cp "backups/${BACKUP_FILE}" \
          "s3://${{ secrets.S3_BACKUP_BUCKET }}/postgresql/${BACKUP_FILE}" \
          --storage-class STANDARD_IA
          
    - name: Upload backup to Azure Blob Storage
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="agrotm_db_${TIMESTAMP}.sql.gz"
        
        az storage blob upload \
          --account-name ${{ secrets.AZURE_STORAGE_ACCOUNT }} \
          --account-key ${{ secrets.AZURE_STORAGE_KEY }} \
          --container-name ${{ secrets.AZURE_CONTAINER_NAME }} \
          --name "postgresql/${BACKUP_FILE}" \
          --file "backups/${BACKUP_FILE}"
      if: ${{ secrets.AZURE_STORAGE_ACCOUNT != '' }}
      
    - name: Upload backup to Google Cloud Storage
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="agrotm_db_${TIMESTAMP}.sql.gz"
        
        gsutil cp "backups/${BACKUP_FILE}" \
          "gs://${{ secrets.GCS_BACKUP_BUCKET }}/postgresql/${BACKUP_FILE}"
      if: ${{ secrets.GCS_BACKUP_BUCKET != '' }}
      
    - name: Clean up old backups (S3)
      run: |
        # Keep only last 30 days of backups
        aws s3 ls "s3://${{ secrets.S3_BACKUP_BUCKET }}/postgresql/" | \
        awk '{print $4}' | \
        sort -r | \
        tail -n +31 | \
        xargs -I {} aws s3 rm "s3://${{ secrets.S3_BACKUP_BUCKET }}/postgresql/{}"
        
    - name: Clean up old backups (Azure)
      run: |
        # Keep only last 30 days of backups
        az storage blob list \
          --account-name ${{ secrets.AZURE_STORAGE_ACCOUNT }} \
          --account-key ${{ secrets.AZURE_STORAGE_KEY }} \
          --container-name ${{ secrets.AZURE_CONTAINER_NAME }} \
          --prefix "postgresql/" | \
        jq -r '.[].name' | \
        sort -r | \
        tail -n +31 | \
        xargs -I {} az storage blob delete \
          --account-name ${{ secrets.AZURE_STORAGE_ACCOUNT }} \
          --account-key ${{ secrets.AZURE_STORAGE_KEY }} \
          --container-name ${{ secrets.AZURE_CONTAINER_NAME }} \
          --name "{}"
      if: ${{ secrets.AZURE_STORAGE_ACCOUNT != '' }}
      
    - name: Clean up old backups (GCS)
      run: |
        # Keep only last 30 days of backups
        gsutil ls "gs://${{ secrets.GCS_BACKUP_BUCKET }}/postgresql/" | \
        sort -r | \
        tail -n +31 | \
        xargs -I {} gsutil rm "{}"
      if: ${{ secrets.GCS_BACKUP_BUCKET != '' }}
      
    - name: Verify backup integrity
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="agrotm_db_${TIMESTAMP}.sql.gz"
        
        # Download and verify backup
        aws s3 cp "s3://${{ secrets.S3_BACKUP_BUCKET }}/postgresql/${BACKUP_FILE}" "verify_${BACKUP_FILE}"
        gunzip -t "verify_${BACKUP_FILE}"
        
        # Test restore to temporary database
        gunzip -c "verify_${BACKUP_FILE}" | \
        psql \
          --host=${{ secrets.DB_HOST }} \
          --port=${{ secrets.DB_PORT }} \
          --username=${{ secrets.DB_USERNAME }} \
          --dbname=postgres \
          --command="CREATE DATABASE backup_test_${TIMESTAMP};"
          
        gunzip -c "verify_${BACKUP_FILE}" | \
        psql \
          --host=${{ secrets.DB_HOST }} \
          --port=${{ secrets.DB_PORT }} \
          --username=${{ secrets.DB_USERNAME }} \
          --dbname="backup_test_${TIMESTAMP}"
          
        # Clean up test database
        psql \
          --host=${{ secrets.DB_HOST }} \
          --port=${{ secrets.DB_PORT }} \
          --username=${{ secrets.DB_USERNAME }} \
          --dbname=postgres \
          --command="DROP DATABASE backup_test_${TIMESTAMP};"
          
        rm "verify_${BACKUP_FILE}"
      env:
        PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        
    - name: Notify success
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#backups'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      if: success()
      
    - name: Notify failure
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#backups'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      if: failure()
      
    - name: Create backup report
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_FILE="agrotm_db_${TIMESTAMP}.sql.gz"
        
        echo "## Backup Report" > backup_report.md
        echo "" >> backup_report.md
        echo "**Date:** $(date)" >> backup_report.md
        echo "**Environment:** ${{ github.event.inputs.environment || 'production' }}" >> backup_report.md
        echo "**Backup File:** ${BACKUP_FILE}" >> backup_report.md
        echo "**Size:** $(du -h backups/${BACKUP_FILE} | cut -f1)" >> backup_report.md
        echo "**Status:** âœ… Success" >> backup_report.md
        echo "" >> backup_report.md
        echo "### Storage Locations:" >> backup_report.md
        echo "- AWS S3: s3://${{ secrets.S3_BACKUP_BUCKET }}/postgresql/${BACKUP_FILE}" >> backup_report.md
        if [ "${{ secrets.AZURE_STORAGE_ACCOUNT }}" != "" ]; then
          echo "- Azure Blob: ${{ secrets.AZURE_STORAGE_ACCOUNT }}/${{ secrets.AZURE_CONTAINER_NAME }}/postgresql/${BACKUP_FILE}" >> backup_report.md
        fi
        if [ "${{ secrets.GCS_BACKUP_BUCKET }}" != "" ]; then
          echo "- Google Cloud: gs://${{ secrets.GCS_BACKUP_BUCKET }}/postgresql/${BACKUP_FILE}" >> backup_report.md
        fi
        
    - name: Upload backup report
      uses: actions/upload-artifact@v3
      with:
        name: backup-report-${{ github.event.inputs.environment || 'production' }}
        path: backup_report.md
        retention-days: 7 
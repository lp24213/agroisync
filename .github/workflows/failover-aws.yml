name: Failover to AWS

on:
  workflow_dispatch:
    inputs:
      reason:
        description: 'Reason for failover'
        required: true
        default: 'Manual failover'
      environment:
        description: 'Target environment'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging

env:
  AWS_REGION: us-east-1
  ECS_CLUSTER: agrotm-cluster
  ECS_SERVICE: agrotm-service
  ROUTE53_HOSTED_ZONE_ID: Z1234567890ABC

jobs:
  failover:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Check current health status
      run: |
        # Check if AWS services are healthy
        echo "Checking AWS service health..."
        
        # Check ECS service status
        SERVICE_STATUS=$(aws ecs describe-services \
          --cluster ${{ env.ECS_CLUSTER }} \
          --services ${{ env.ECS_SERVICE }} \
          --query 'services[0].status' \
          --output text)
          
        if [ "$SERVICE_STATUS" != "ACTIVE" ]; then
          echo "AWS ECS service is not healthy: $SERVICE_STATUS"
          exit 1
        fi
        
        # Check RDS database status
        DB_STATUS=$(aws rds describe-db-instances \
          --db-instance-identifier agrotm-db \
          --query 'DBInstances[0].DBInstanceStatus' \
          --output text)
          
        if [ "$DB_STATUS" != "available" ]; then
          echo "AWS RDS database is not healthy: $DB_STATUS"
          exit 1
        fi
        
        echo "AWS services are healthy"
        
    - name: Scale up AWS services
      run: |
        # Scale up ECS service
        aws ecs update-service \
          --cluster ${{ env.ECS_CLUSTER }} \
          --service ${{ env.ECS_SERVICE }} \
          --desired-count 5
          
        # Scale up RDS if needed
        aws rds modify-db-instance \
          --db-instance-identifier agrotm-db \
          --apply-immediately
          
    - name: Update DNS to point to AWS
      run: |
        # Create DNS change batch
        cat > dns-change.json << EOF
        {
          "Changes": [
            {
              "Action": "UPSERT",
              "ResourceRecordSet": {
                "Name": "www.agrotm.com",
                "Type": "A",
                "TTL": 300,
                "ResourceRecords": [
                  {
                    "Value": "${{ secrets.AWS_APP_IP }}"
                  }
                ]
              }
            },
            {
              "Action": "UPSERT",
              "ResourceRecordSet": {
                "Name": "api.agrotm.com",
                "Type": "A",
                "TTL": 300,
                "ResourceRecords": [
                  {
                    "Value": "${{ secrets.AWS_API_IP }}"
                  }
                ]
              }
            }
          ]
        }
        EOF
        
        # Apply DNS changes
        aws route53 change-resource-record-sets \
          --hosted-zone-id ${{ env.ROUTE53_HOSTED_ZONE_ID }} \
          --change-batch file://dns-change.json
          
    - name: Update CloudFront distribution
      run: |
        # Invalidate CloudFront cache
        aws cloudfront create-invalidation \
          --distribution-id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }} \
          --paths "/*"
          
    - name: Wait for DNS propagation
      run: |
        echo "Waiting for DNS propagation..."
        sleep 60
        
        # Check if DNS is propagated
        for i in {1..10}; do
          echo "Checking DNS propagation... ($i/10)"
          
          if dig +short www.agrotm.com | grep -q "${{ secrets.AWS_APP_IP }}"; then
            echo "DNS propagation successful"
            break
          fi
          
          sleep 30
        done
        
    - name: Run health checks
      run: |
        # Wait for application to be healthy
        sleep 30
        
        # Run health checks
        curl -f https://www.agrotm.com/health || exit 1
        curl -f https://api.agrotm.com/health || exit 1
        
        # Run smoke tests
        npm run test:smoke
        
    - name: Monitor failover
      run: |
        # Monitor failover for 10 minutes
        for i in {1..60}; do
          echo "Monitoring failover... ($i/60)"
          
          # Check application health
          HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://www.agrotm.com/health)
          
          if [ "$HEALTH_STATUS" = "200" ]; then
            echo "Application is healthy"
            break
          fi
          
          sleep 10
        done
        
        if [ "$HEALTH_STATUS" != "200" ]; then
          echo "Application failed health checks"
          exit 1
        fi
        
    - name: Update monitoring systems
      run: |
        # Update monitoring systems to use AWS endpoints
        curl -X POST ${{ secrets.MONITORING_WEBHOOK_URL }} \
          -H "Content-Type: application/json" \
          -d "{\"action\": \"failover\", \"provider\": \"aws\", \"environment\": \"${{ github.event.inputs.environment || 'production' }}\", \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}"
          
    - name: Create failover record
      run: |
        # Create failover record in monitoring system
        aws logs put-log-events \
          --log-group-name "/aws/ecs/agrotm-failovers" \
          --log-stream-name "$(date +%Y-%m-%d)" \
          --log-events timestamp=$(date +%s)000,message="Failover to AWS successful: ${{ github.event.inputs.reason }}"
          
    - name: Notify failover success
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#failover'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        text: |
          ✅ Failover to AWS successful!
          Environment: ${{ github.event.inputs.environment || 'production' }}
          Reason: ${{ github.event.inputs.reason }}
          URL: https://www.agrotm.com
      if: success()
      
    - name: Notify failover failure
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#failover'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        text: |
          ❌ Failover to AWS failed!
          Environment: ${{ github.event.inputs.environment || 'production' }}
          Reason: ${{ github.event.inputs.reason }}
          Check the logs for details.
      if: failure()
      
    - name: Rollback on failure
      run: |
        # Rollback DNS changes if failover fails
        cat > dns-rollback.json << EOF
        {
          "Changes": [
            {
              "Action": "UPSERT",
              "ResourceRecordSet": {
                "Name": "www.agrotm.com",
                "Type": "A",
                "TTL": 300,
                "ResourceRecords": [
                  {
                    "Value": "${{ secrets.PRIMARY_APP_IP }}"
                  }
                ]
              }
            },
            {
              "Action": "UPSERT",
              "ResourceRecordSet": {
                "Name": "api.agrotm.com",
                "Type": "A",
                "TTL": 300,
                "ResourceRecords": [
                  {
                    "Value": "${{ secrets.PRIMARY_API_IP }}"
                  }
                ]
              }
            }
          ]
        }
        EOF
        
        aws route53 change-resource-record-sets \
          --hosted-zone-id ${{ env.ROUTE53_HOSTED_ZONE_ID }} \
          --change-batch file://dns-rollback.json
          
        echo "Rolled back DNS changes"
      if: failure() 